{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome \n",
    "\n",
    "This is mission 62 from dataquest : https://www.dataquest.io/mission/62/customizing-functions-and-debugging-errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'frmer', '', 'prabably', 'world', '', 'decidid', 'grw', 'kniw', 'hd', 'goe', '', 'aroudn', '', 'almosty', 'a', 'perserved', '', 'stoped', '', 'crzy', '10', '', 'undetered', 'alternatng', '', 'a', 'mich', '']\n"
     ]
    }
   ],
   "source": [
    "f = open(\"story.txt\", 'r')\n",
    "story_string = f.read()\n",
    "vocabulary = open(\"dictionary.txt\", \"r\").read()\n",
    "\n",
    "def clean_text(text_string, special_characters):\n",
    "    cleaned_string = text_string\n",
    "    for string in special_characters:\n",
    "        cleaned_string = cleaned_string.replace(string, \"\")\n",
    "    cleaned_string = cleaned_string.lower()\n",
    "    return(cleaned_string)\n",
    "\n",
    "def tokenize(text_string, special_characters):\n",
    "    cleaned_story = clean_text(text_string, special_characters)\n",
    "    story_tokens = cleaned_story.split(\" \")\n",
    "    return(story_tokens)\n",
    "\n",
    "misspelled_words = []\n",
    "clean_chars = [\",\", \".\", \"'\", \";\", \"\\n\"]\n",
    "tokenized_story = tokenize(story_string, clean_chars)\n",
    "tokenized_vocabulary = tokenize(vocabulary, clean_chars)\n",
    "\n",
    "for ts in tokenized_story:\n",
    "    if ts not in tokenized_vocabulary:\n",
    "        misspelled_words.append(ts)\n",
    "print(misspelled_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify the tokenize() function:\n",
    "#### Use an if statement to check whether clean is True. If so:\n",
    "    Clean text_string using clean_text, and assign the returned string to a variable.\n",
    "    Tokenize the new string variable using the split() method, and assign the returned list to another new variable.\n",
    "    Return this list.\n",
    "#### Outside the if statement, write the code we want executed if clean is False:\n",
    "    Tokenize text_string using the split() method, and assign the returned list to a new variable.\n",
    "    Return this list.\n",
    "### Outside the tokenize() function:\n",
    "#### Use the tokenize() function to clean and tokenize story_string, and assign the result to tokenized_story.\n",
    "#### Use the tokenize() function to tokenize vocabulary, and assign the result to tokenized_vocabulary.\n",
    "#### Finally, loop over each element in tokenized_story and check whether it exists in tokenized_vocabulary. If it doesn't, add it to misspelled_words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['frmer', '', 'prabably', '', 'decidid', 'grw', 'kniw', 'hd', 'goe', '', 'aroudn', '', 'almosty', 'perserved', '', 'stoped', '', 'crzy', '10', '', 'undetered', 'alternatng', '', 'mich', '']\n"
     ]
    }
   ],
   "source": [
    "# Default code\n",
    "def clean_text(text_string, special_characters):\n",
    "    cleaned_string = text_string\n",
    "    for string in special_characters:\n",
    "        cleaned_string = cleaned_string.replace(string, \"\")\n",
    "    cleaned_string = cleaned_string.lower()\n",
    "    return(cleaned_string)\n",
    "\n",
    "def tokenize(text_string, special_characters, clean=False):\n",
    "    if(clean==True):\n",
    "        cleaned_story = clean_text(text_string, special_characters)\n",
    "        story_tokens = cleaned_story.split(\" \")\n",
    "        return(story_tokens)\n",
    "    story_tokens = text_string.split(\" \")\n",
    "    return(story_tokens)\n",
    "\n",
    "\n",
    "story_string = open(\"story.txt\",\"r\").read()\n",
    "vocabulary = open(\"dictionary.txt\", \"r\").read()\n",
    "clean_chars = [\",\", \".\", \"'\", \";\", \"\\n\"]\n",
    "\n",
    "tokenized_story = []\n",
    "tokenized_story = tokenize(story_string, clean_chars, True)\n",
    "tokenized_vocabulary = []\n",
    "tokenized_vocabulary = tokenize(vocabulary, clean_chars)\n",
    "misspelled_words = []\n",
    "for ts in tokenized_story:\n",
    "    if ts not in tokenized_vocabulary:\n",
    "        misspelled_words.append(ts)\n",
    "print(misspelled_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore named arguments by uncommenting the starter code and running the different function calls.\n",
    "#### Use the print() function to verify the returned values are the same across the different function calls.\n",
    "#### Click Next once you're done exploring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['There', 'was', 'once', 'a', 'great', 'and', 'noble', 'frmer', 'named', 'Julius.']\n",
      "['There', 'was', 'once', 'a', 'great', 'and', 'noble', 'frmer', 'named', 'Julius.']\n",
      "['There', 'was', 'once', 'a', 'great', 'and', 'noble', 'frmer', 'named', 'Julius.']\n",
      "['a', 'about', 'after', 'almost', 'along', 'alternating', 'an', 'and', 'anguish', 'around']\n",
      "['a', 'about', 'after', 'almost', 'along', 'alternating', 'an', 'and', 'anguish', 'around']\n"
     ]
    }
   ],
   "source": [
    "clean_chars = [\",\", \".\", \"'\", \";\", \"\\n\"]\n",
    "\n",
    "# These three lines represent different ways of expressing the same function call.\n",
    "tokenized_story = tokenize(clean=False, text_string = story_string, special_characters = clean_chars)\n",
    "print(tokenized_story[0:10])\n",
    "tokenized_story = tokenize(text_string = story_string, clean=False, special_characters = clean_chars)\n",
    "print(tokenized_story[0:10])\n",
    "tokenized_story = tokenize(special_characters = clean_chars, text_string = story_string, clean=False)\n",
    "print(tokenized_story[0:10])\n",
    "\n",
    "# These two lines represent different ways of expressing the same function call.\n",
    "tokenized_vocabulary = tokenize(text_string=vocabulary, special_characters=clean_chars)\n",
    "print(tokenized_vocabulary[0:10])\n",
    "tokenized_vocabulary = tokenize(special_characters=clean_chars, text_string=vocabulary)\n",
    "print(tokenized_vocabulary[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a function called spell_check():\n",
    "#### Include the following arguments:\n",
    "    vocabulary_file - the location of the vocabulary text file\n",
    "    text_file - the location of the text we want to spell check\n",
    "    special_characters - with the default value of the list [\",\",\".\",\"'\",\";\",\"\\n\"].\n",
    "#### In the function body:\n",
    "    Create an empty list and assign it to misspelled_words.\n",
    "    Read both files into strings using the open() and read() functions: open(vocabulary_file).read()\n",
    "    Call the tokenize() function to tokenize the string containing the vocabulary. Assign the result to tokenized_vocabulary.\n",
    "    Call the tokenize() function to clean and tokenize the string containing the text we want to spell check. Assign the result to tokenized_text.\n",
    "    Write a for loop that iterates over tokenized_text. For each token in tokenized_text:\n",
    "        Write an if statement that checks whether the token isn't in tokenized_vocabulary, and if it's not equal to ''.\n",
    "        If it meets both criteria, append the token to misspelled_words.\n",
    "    Outside the for loop, return misspelled_words\n",
    "    \n",
    "### After we've written the function, call spell_check:\n",
    "    Pass in story.txt as the text_file parameter.\n",
    "    Pass in dictionary.txt as the vocabulary_file parameter.\n",
    "    Use the default value for special_characters.\n",
    "    Assign the result to final_misspelled_words.\n",
    "    Use the print() function to display final_misspelled_words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['frmer', 'prabably', 'decidid', 'grw', 'kniw', 'hd', 'goe', 'aroudn', 'almosty', 'perserved', 'stoped', 'crzy', '10', 'undetered', 'alternatng', 'mich']\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text_string, special_characters):\n",
    "    cleaned_string = text_string\n",
    "    for string in special_characters:\n",
    "        cleaned_string = cleaned_string.replace(string, \"\")\n",
    "    cleaned_string = cleaned_string.lower()\n",
    "    return(cleaned_string)\n",
    "\n",
    "def tokenize(text_string, special_characters, clean=False):\n",
    "    cleaned_text = text_string\n",
    "    if clean:\n",
    "        cleaned_text = clean_text(text_string, special_characters)\n",
    "    tokens = cleaned_text.split(\" \")\n",
    "    return(tokens)\n",
    "\n",
    "def spell_check(vocabulary_file, text_file, special_characters=[\",\",\".\",\"'\",\";\",\"\\n\"]):\n",
    "    misspelled_words = []\n",
    "    \n",
    "    vocabulary = open(vocabulary_file).read()\n",
    "    text = open(text_file).read()\n",
    "    \n",
    "    tokenized_vocabulary = tokenize(vocabulary, special_characters)\n",
    "    \n",
    "    tokenized_text = tokenize(text, special_characters, True)\n",
    "    \n",
    "    for ts in tokenized_text:\n",
    "        if ( (ts in tokenized_vocabulary)==False and ts!=''):\n",
    "            misspelled_words.append(ts)\n",
    "    return(misspelled_words)\n",
    "    \n",
    "    \n",
    "    \n",
    "final_misspelled_words = []\n",
    "final_misspelled_words = spell_check(text_file=\"story.txt\", vocabulary_file=\"dictionary.txt\")\n",
    "print(final_misspelled_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The starter code contains multiple syntax errors. Scan and edit the code to resolve these errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-20-8c7f31af553c>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-20-8c7f31af553c>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    def spell_check(vocabulary_file, text_file, special_characters=[\",\",\".\",\"'\",\";\",\"\\n\"):\u001b[0m\n\u001b[0m                                                                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def spell_check(vocabulary_file, text_file, special_characters=[\",\",\".\",\"'\",\";\",\"\\n\"):\n",
    "    misspelled_words = []\n",
    "    vocabulary = open(vocabulary_file).read()\n",
    "    text = open(text_file.read()\n",
    "    \n",
    "    tokenized_vocabulary = tokenize(vocabulary, special_characters)\n",
    "    tokenized_text = tokenize(text, special_characters, True)\n",
    "    \n",
    "    for ts in tokenized_text:\n",
    "        if ts not in tokenized_vocabulary and ts != '':\n",
    "            misspelled_words.append(ts)\n",
    "    return(misspelled_words)\n",
    "\n",
    "final_misspelled_words = spell_check(vocabulary_file=\"dictionary.txt\", text_file=\"story.txt\")\n",
    "print(final_misspelled_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### he code cell contains the sample code from this step. Experiment with it to explore other runtime errors.\n",
    "#### You could try:\n",
    "    Concatenating an integer to a string, instead of the other way around\n",
    "    Casting \"guardians\" into an integer instead using the int() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "must be str, not int",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-78bd43be38bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mforty_two\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"42\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mforty_two\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m42\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"guardians\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: must be str, not int"
     ]
    }
   ],
   "source": [
    "forty_two = \"42\"\n",
    "forty_two + 42\n",
    "\n",
    "int(\"guardians\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Edit the default code and remove the error.\n",
    "#### Don't set a value for the special_characters argument. Instead, let the spell_check() function use the default value for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['frmer', 'prabably', 'decidid', 'grw', 'kniw', 'hd', 'goe', 'aroudn', 'almosty', 'perserved', 'stoped', 'crzy', '10', 'undetered', 'alternatng', 'mich']\n"
     ]
    }
   ],
   "source": [
    "def spell_check(vocabulary_file, text_file, special_characters=[\",\",\".\",\"'\",\";\",\"\\n\"]):\n",
    "    misspelled_words = []\n",
    "    special_characters=[\",\",\".\",\"'\",\";\",\"\\n\"]\n",
    "    vocabulary = open(vocabulary_file).read()\n",
    "    # Add ending parentheses.\n",
    "    text = open(text_file).read()\n",
    "    \n",
    "    # Fix indentation.\n",
    "    tokenized_vocabulary = tokenize(vocabulary, special_characters)\n",
    "    tokenized_text = tokenize(text, special_characters, True)\n",
    "    \n",
    "    for ts in tokenized_text:\n",
    "        if ts not in tokenized_vocabulary and ts != '':\n",
    "            misspelled_words.append(ts)\n",
    "    return(misspelled_words)\n",
    "\n",
    "final_misspelled_words = spell_check(vocabulary_file=\"dictionary.txt\", text_file=\"story.txt\", special_characters=1)\n",
    "print(final_misspelled_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THIS IS THE END 8-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
